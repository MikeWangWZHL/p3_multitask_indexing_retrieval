{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    default_data_collator,\n",
    ")\n",
    "from promptsource.templates import DatasetTemplates\n",
    "\n",
    "import sys\n",
    "ROOT_DIR = \"/cephfs/user/mikeeewang/summer_22/code/t-zero\"\n",
    "T0_DIR = os.path.join(ROOT_DIR,'t0')\n",
    "sys.path.insert(1, T0_DIR)\n",
    "from data_collator import DataCollatorForMultipleChoice\n",
    "from model import ModelBase, ModelBase_with_confidence\n",
    "\n",
    "\n",
    "from template_list import template_list\n",
    "from retrieval import setup_retriever, retrieve, setup_retriever_shard\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "output_dir = \"./tmp_output\"\n",
    "dataset_name = \"super_glue\"\n",
    "dataset_config_name = \"wic\"\n",
    "model_name_or_path = \"bigscience/T0\"\n",
    "model_name_or_path = \"bigscience/T0_3B\"\n",
    "config_name = None\n",
    "template_name = None\n",
    "\n",
    "\n",
    "debug = False\n",
    "use_slow_tokenizer = False\n",
    "tokenizer_name = False\n",
    "per_device_eval_batch_size = 8\n",
    "if_parallelize = True\n",
    "pad_to_max_length = False\n",
    "eval_all_templates = True\n",
    "max_length = 1024\n",
    "target_max_length = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/07/2022 17:03:39 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "Mixed precision type: no\n",
      "\n",
      "06/07/2022 17:03:40 - WARNING - datasets.builder - Reusing dataset super_glue (/data2/mikeeewang/.cache/huggingface/super_glue/wic/1.0.2/d040c658e2ddef6934fdd97deb45c777b6ff50c524781ea434e7219b56a428a7)\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /data2/mikeeewang/.cache/huggingface/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /data2/mikeeewang/.cache/huggingface/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/spiece.model from cache at /data2/mikeeewang/.cache/huggingface/d8c957338a9c967898a57f364d17f1fc0b7e514780dbdd99eb5a6306cf6d9ad4.d6f0605ae3d57070be74b4c12206072ab332922acff822e6b5458691dbda7551\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/special_tokens_map.json from cache at /data2/mikeeewang/.cache/huggingface/303fbee39a17e96552ac07e02b70ba62ff0ad760609687e3a1b92b4ad2dff58c.c94798918c92ded6aeef2d2f0e666d2cc4145eca1aa6e1336fde07f2e13e2f46\n",
      "loading file https://huggingface.co/bigscience/T0_3B/resolve/main/tokenizer_config.json from cache at /data2/mikeeewang/.cache/huggingface/2c9b4442b8c3ca21f0457cbd7b8e4705058d02c93336a0450d020dafc2abb4d3.b1a2e3c152960fdc6b3d16520fa9f1591e2818d7dd66946c219e651f224894bf\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /data2/mikeeewang/.cache/huggingface/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bigscience/T0_3B/resolve/main/config.json from cache at /data2/mikeeewang/.cache/huggingface/7b128e6b48089ae556964fea17b39635abd0124e77f8fa30267896af500a4d6d.a54ecffc6881ea8ae0af8a0dca40a7bcd51ccf51d434d2f7d0569844f6fb1c60\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"bigscience/T0_3B\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 5120,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 2048,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"num_decoder_layers\": 24,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "06/07/2022 17:03:44 - INFO - model - Building EncoderDecoderModel\n",
      "loading weights file https://huggingface.co/bigscience/T0_3B/resolve/main/pytorch_model.bin from cache at /data2/mikeeewang/.cache/huggingface/a80e28e34bce4ce1d72ae1fcbb46861412498adb5ab95928e3344ddfc5481524.d53f6a5f906212dee199edcde17c3c43695656c435962f2dc1636562577598bb\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at bigscience/T0_3B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done loading model\n",
      "evaluating all possible templates, total number:10\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator()\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "# accelerator.is_local_main_process is only True for one process per machine.\n",
    "logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n",
    "if accelerator.is_local_main_process:\n",
    "    datasets.utils.logging.set_verbosity_warning()\n",
    "    transformers.utils.logging.set_verbosity_info()\n",
    "else:\n",
    "    datasets.utils.logging.set_verbosity_error()\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "# Handle the output directory creation\n",
    "if accelerator.is_main_process:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "accelerator.wait_for_everyone()\n",
    "\n",
    "# In distributed evaluation, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "\n",
    "if dataset_name is not None:\n",
    "    # Downloading and loading a dataset from the hub.\n",
    "    if dataset_name == \"anli\":\n",
    "        raw_datasets = load_dataset(dataset_name, split=dataset_config_name)\n",
    "    else:\n",
    "        raw_datasets = load_dataset(dataset_name, dataset_config_name, split=\"validation\")\n",
    "\n",
    "# Trim a number of evaluation examples\n",
    "if debug:\n",
    "    raw_datasets = raw_datasets.select(range(100))\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "if config_name:\n",
    "    config = AutoConfig.from_pretrained(config_name)\n",
    "elif model_name_or_path:\n",
    "    config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Either `args.config_name` or `args.model_name_or_path` should be provided.\"\n",
    "    )\n",
    "\n",
    "if tokenizer_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=not use_slow_tokenizer)\n",
    "elif model_name_or_path:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=not use_slow_tokenizer)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n",
    "        \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n",
    "    )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    for token in [tokenizer.eos_token, tokenizer.bos_token, tokenizer.sep_token]:\n",
    "        if token is not None:\n",
    "            tokenizer.pad_token = token\n",
    "    if tokenizer.pad_token is None:\n",
    "        raise ValueError(\"Please define a pad token id.\")\n",
    "\n",
    "\n",
    "model = ModelBase_with_confidence.from_config(\n",
    "    config=config,\n",
    "    model_name_or_path=model_name_or_path,\n",
    "    parallelize=if_parallelize\n",
    ")\n",
    "print('done loading model')\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "padding = \"max_length\" if pad_to_max_length else False\n",
    "\n",
    "\n",
    "# Get the prompt to apply and the possible targets.\n",
    "prompts = DatasetTemplates(\n",
    "    f\"{dataset_name}\"\n",
    "    if dataset_config_name is None\n",
    "    else f\"{dataset_name}/{dataset_config_name}\"\n",
    ")\n",
    "\n",
    "assert (dataset_name, dataset_config_name) in template_list\n",
    "\n",
    "if eval_all_templates:\n",
    "    template_names = template_list[(dataset_name, dataset_config_name)]\n",
    "    print(f'evaluating all possible templates, total number:{len(template_names)}')\n",
    "else:\n",
    "    template_names = [template_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating tempalte question-context-meaning-with-label ...\n",
      "preparing dataset ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213b1015b29a46058c78dd4e466a9252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/07/2022 17:06:07 - INFO - __main__ - Sample 379 of the training set: {'input_ids': [[3520, 8, 1448, 96, 27673, 121, 43, 8, 337, 2530, 16, 175, 192, 16513, 58, 2163, 6, 465, 58, 216, 1790, 46, 7544, 3211, 30, 82, 161, 5, 389, 3211, 13, 28582, 5], [3520, 8, 1448, 96, 27673, 121, 43, 8, 337, 2530, 16, 175, 192, 16513, 58, 2163, 6, 465, 58, 216, 1790, 46, 7544, 3211, 30, 82, 161, 5, 389, 3211, 13, 28582, 5]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[465, 1], [2163, 1]], 'labels_attention_mask': [[1, 1], [1, 1]], 'targets': 0, 'indices': 379}.\n",
      "06/07/2022 17:06:07 - INFO - __main__ - Sample 485 of the training set: {'input_ids': [[3520, 8, 1448, 96, 35, 15299, 15, 121, 43, 8, 337, 2530, 16, 175, 192, 16513, 58, 2163, 6, 465, 58, 695, 15299, 15, 3, 9, 2068, 5, 695, 15299, 15, 3, 9, 4550, 5], [3520, 8, 1448, 96, 35, 15299, 15, 121, 43, 8, 337, 2530, 16, 175, 192, 16513, 58, 2163, 6, 465, 58, 695, 15299, 15, 3, 9, 2068, 5, 695, 15299, 15, 3, 9, 4550, 5]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[465, 1], [2163, 1]], 'labels_attention_mask': [[1, 1], [1, 1]], 'targets': 0, 'indices': 485}.\n",
      "06/07/2022 17:06:07 - INFO - __main__ - Sample 465 of the training set: {'input_ids': [[3520, 8, 1448, 96, 7197, 49, 162, 121, 43, 8, 337, 2530, 16, 175, 192, 16513, 58, 2163, 6, 465, 58, 1266, 3473, 15, 8, 3065, 16, 8, 384, 5, 304, 8996, 10844, 5], [3520, 8, 1448, 96, 7197, 49, 162, 121, 43, 8, 337, 2530, 16, 175, 192, 16513, 58, 2163, 6, 465, 58, 1266, 3473, 15, 8, 3065, 16, 8, 384, 5, 304, 8996, 10844, 5]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[465, 1], [2163, 1]], 'labels_attention_mask': [[1, 1], [1, 1]], 'targets': 1, 'indices': 465}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Does the word \"class\" have the same meaning in these two sentences? Yes, No?\\nAn emerging professional class.\\nApologizing for losing your temper, even though you were badly provoked, showed real class.', 'target': 'No', 'answer_choices': ['No', 'Yes']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/07/2022 17:06:08 - INFO - __main__ - ***** Running step one for mining confident examples *****\n",
      "06/07/2022 17:06:08 - INFO - __main__ -   Num examples = 638\n",
      "06/07/2022 17:06:08 - INFO - __main__ -   Instantaneous batch size per device = 8\n",
      "06/07/2022 17:06:08 - INFO - __main__ -   Total eval batch size (w. parallel, distributed) = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538bba9decc0405e94a087435459e175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "div(): argument 'input' (position 1) must be Tensor, not torch.return_types.max",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/self_referencing.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/self_referencing.ipynb#ch0000004vscode-remote?line=133'>134</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m eval_dataloader:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/self_referencing.ipynb#ch0000004vscode-remote?line=134'>135</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/self_referencing.ipynb#ch0000004vscode-remote?line=135'>136</a>\u001b[0m         predictions, confidences \u001b[39m=\u001b[39m model(batch)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/self_referencing.ipynb#ch0000004vscode-remote?line=137'>138</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(batch[\u001b[39m'\u001b[39m\u001b[39mindices\u001b[39m\u001b[39m'\u001b[39m])):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/self_referencing.ipynb#ch0000004vscode-remote?line=138'>139</a>\u001b[0m         idx \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(batch[\u001b[39m'\u001b[39m\u001b[39mindices\u001b[39m\u001b[39m'\u001b[39m][i])\n",
      "File \u001b[0;32m/data2/mikeeewang/miniconda3/envs/t-zero/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///data2/mikeeewang/miniconda3/envs/t-zero/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///data2/mikeeewang/miniconda3/envs/t-zero/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///data2/mikeeewang/miniconda3/envs/t-zero/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///data2/mikeeewang/miniconda3/envs/t-zero/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///data2/mikeeewang/miniconda3/envs/t-zero/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///data2/mikeeewang/miniconda3/envs/t-zero/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///data2/mikeeewang/miniconda3/envs/t-zero/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/cephfs/user/mikeeewang/summer_22/code/t-zero/t0/model.py:184\u001b[0m, in \u001b[0;36mEncoderDecoderModel_with_confidence.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    <a href='file:///cephfs/user/mikeeewang/summer_22/code/t-zero/t0/model.py?line=181'>182</a>\u001b[0m max_ \u001b[39m=\u001b[39m seq_log_prob\u001b[39m.\u001b[39mmax(dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///cephfs/user/mikeeewang/summer_22/code/t-zero/t0/model.py?line=182'>183</a>\u001b[0m min_ \u001b[39m=\u001b[39m seq_log_prob\u001b[39m.\u001b[39mmin(dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> <a href='file:///cephfs/user/mikeeewang/summer_22/code/t-zero/t0/model.py?line=183'>184</a>\u001b[0m confidence \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdiv(max_, min_)\n\u001b[1;32m    <a href='file:///cephfs/user/mikeeewang/summer_22/code/t-zero/t0/model.py?line=185'>186</a>\u001b[0m predictions \u001b[39m=\u001b[39m seq_log_prob\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///cephfs/user/mikeeewang/summer_22/code/t-zero/t0/model.py?line=186'>187</a>\u001b[0m \u001b[39m# print(predictions)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: div(): argument 'input' (position 1) must be Tensor, not torch.return_types.max"
     ]
    }
   ],
   "source": [
    "\n",
    "template_names = template_names[:1] # for debugging\n",
    "# main loop over templates\n",
    "all_results_step1 = []\n",
    "all_results_step2 = []\n",
    "for template_name in template_names:\n",
    "    \n",
    "    idx_to_data = {}\n",
    "\n",
    "    print(f'evaluating tempalte {template_name} ...')\n",
    "    template = prompts[template_name]\n",
    "    column_names = raw_datasets.column_names\n",
    "    \n",
    "    #####################################\n",
    "    # step 1 finding confident examples #\n",
    "    #####################################\n",
    "\n",
    "    ### preprocess dataset functions step 1 ###\n",
    "    def preprocess_function_step1(examples):\n",
    "        bs = len(examples[column_names[0]])\n",
    "\n",
    "        input_texts = []\n",
    "        target_texts = []\n",
    "        indices = []\n",
    "        answer_choices_texts = []\n",
    "        for i in range(bs):\n",
    "            ex = {\n",
    "                k: examples[k][i]\n",
    "                for k in column_names\n",
    "            }\n",
    "            input, target = template.apply(ex)\n",
    "            ex_answer_choices = template.get_answer_choices_list(ex)\n",
    "            assert target in ex_answer_choices\n",
    "            input_texts.append(input)\n",
    "            target_texts.append(target)\n",
    "            answer_choices_texts.append(ex_answer_choices)\n",
    "\n",
    "            idx = len(idx_to_data)\n",
    "            idx_to_data[str(idx)] = {\"input\": input, \"target\": target, \"answer_choices\":ex_answer_choices}\n",
    "            indices.append(idx)\n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            input_texts,\n",
    "            padding=padding,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        tokenized_targets = [\n",
    "            tokenizer(\n",
    "                ans_choi,\n",
    "                padding=True,\n",
    "                max_length=target_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for ans_choi in answer_choices_texts\n",
    "        ]\n",
    "\n",
    "        features = {\n",
    "            k: [\n",
    "                [elem for _ in range(len(tokenized_targets[idx][\"input_ids\"]))]\n",
    "                for idx, elem in enumerate(v)\n",
    "            ]\n",
    "            for k, v in tokenized_inputs.items()\n",
    "        }\n",
    "\n",
    "        features[\"labels\"] = [\n",
    "            tokenized_targets[idx][\"input_ids\"]\n",
    "            for idx in range(bs)\n",
    "        ]\n",
    "        features[\"labels_attention_mask\"] = [\n",
    "            tokenized_targets[idx][\"attention_mask\"]\n",
    "            for idx in range(bs)\n",
    "        ]\n",
    "        features[\"targets\"] = [\n",
    "            answer_choices_texts[idx].index(t)\n",
    "            for idx, t in enumerate(target_texts)\n",
    "        ]\n",
    "        features['indices'] = [\n",
    "            indices[idx] for idx in range(bs)\n",
    "        ]\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    ### preprocess dataset ###\n",
    "    with accelerator.main_process_first():\n",
    "        print('preparing dataset ...')\n",
    "        eval_dataset = raw_datasets.map(\n",
    "            preprocess_function_step1, batched=True, remove_columns=column_names\n",
    "        )\n",
    "        \n",
    "    print(idx_to_data['0'])\n",
    "    # Log a few random samples from the eval set:\n",
    "    for index in random.sample(range(len(eval_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {eval_dataset[index]}.\")\n",
    "\n",
    "    # DataLoaders creation:\n",
    "    if pad_to_max_length:\n",
    "        # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "        # to tensors.\n",
    "        data_collator = default_data_collator\n",
    "    else:\n",
    "        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "        data_collator = DataCollatorForMultipleChoice(\n",
    "            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "        )\n",
    "    \n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=per_device_eval_batch_size)\n",
    "\n",
    "\n",
    "    # Use the device given by the `accelerator` object.\n",
    "    if not if_parallelize:\n",
    "        model.to(accelerator.device)\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    eval_dataloader = accelerator.prepare(eval_dataloader)\n",
    "\n",
    "    # Metrics\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "    # Eval!\n",
    "    total_batch_size = per_device_eval_batch_size * accelerator.num_processes\n",
    "\n",
    "    logger.info(\"***** Running step one for mining confident examples *****\")\n",
    "    logger.info(f\"  Num examples = {len(eval_dataset)}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {per_device_eval_batch_size}\")\n",
    "    logger.info(f\"  Total eval batch size (w. parallel, distributed) = {total_batch_size}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(len(eval_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            predictions, confidences = model(batch)\n",
    "\n",
    "        for i in range(len(batch['indices'])):\n",
    "            idx = int(batch['indices'][i])\n",
    "            pred = predictions[i].detach().cpu().numpy()\n",
    "            confidence = confidences[i].detach().cpu().numpy()\n",
    "            idx_to_data[str(idx)]['prediction'] = pred\n",
    "            idx_to_data[str(idx)]['confidence'] = confidence\n",
    "\n",
    "        metric.add_batch(\n",
    "            predictions=accelerator.gather(predictions),\n",
    "            references=accelerator.gather(batch[\"targets\"]),\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    accelerator.print(f\"Result step one: {eval_metric}\")\n",
    "\n",
    "    results = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"dataset_config_name\": dataset_config_name,\n",
    "        \"template_name\": template_name,\n",
    "        \"evaluation\": eval_metric,\n",
    "        \"retrieval_database\": None\n",
    "    }\n",
    "    all_results_step1.append(results)\n",
    "\n",
    "    ###############################################\n",
    "    # step 2 augment with most confident examples #\n",
    "    ###############################################\n",
    "\n",
    "    # find confident examples \n",
    "    cands = [(key,value['confidence']) for key, value in idx_to_data.items()]\n",
    "    cands = sorted(cands, key = lambda x: x[1], reverse = True)\n",
    "    cands = cands[:10] # take top 10\n",
    "    aug_data_points = [idx_to_data[str(x[0])] for x in cands]\n",
    "    # TODO use retrieval for this\n",
    "    print(aug_data_points)\n",
    "    # TODO\n",
    "    concat_num = 1\n",
    "\n",
    "    ### preprocess dataset functions step 2 ###\n",
    "    def preprocess_function_step2(examples):\n",
    "        bs = len(examples[column_names[0]])\n",
    "\n",
    "        input_texts = []\n",
    "        target_texts = []\n",
    "        indices = []\n",
    "        answer_choices_texts = []\n",
    "        for i in range(bs):\n",
    "            ex = {\n",
    "                k: examples[k][i]\n",
    "                for k in column_names\n",
    "            }\n",
    "            input, target = template.apply(ex)\n",
    "            ex_answer_choices = template.get_answer_choices_list(ex)\n",
    "            assert target in ex_answer_choices\n",
    "            \n",
    "            # augment with self examples\n",
    "            picked_augs = random.choices(aug_data_points, k=concat_num)\n",
    "            prefix = \"\"\n",
    "            for ii in range(concat_num):\n",
    "                picked_item = picked_augs[ii]\n",
    "                prefix = prefix + picked_item['input'] + picked_item['answer_choices'][int(picked_item['prediction'])]\n",
    "                prefix = prefix.rstrip('\\n')\n",
    "                prefix += '\\n\\n'\n",
    "            input = prefix + input\n",
    "\n",
    "            input_texts.append(input)\n",
    "            target_texts.append(target)\n",
    "            answer_choices_texts.append(ex_answer_choices)\n",
    "            \n",
    "\n",
    "        tokenized_inputs = tokenizer(\n",
    "            input_texts,\n",
    "            padding=padding,\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        tokenized_targets = [\n",
    "            tokenizer(\n",
    "                ans_choi,\n",
    "                padding=True,\n",
    "                max_length=target_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for ans_choi in answer_choices_texts\n",
    "        ]\n",
    "\n",
    "        features = {\n",
    "            k: [\n",
    "                [elem for _ in range(len(tokenized_targets[idx][\"input_ids\"]))]\n",
    "                for idx, elem in enumerate(v)\n",
    "            ]\n",
    "            for k, v in tokenized_inputs.items()\n",
    "        }\n",
    "\n",
    "        features[\"labels\"] = [\n",
    "            tokenized_targets[idx][\"input_ids\"]\n",
    "            for idx in range(bs)\n",
    "        ]\n",
    "        features[\"labels_attention_mask\"] = [\n",
    "            tokenized_targets[idx][\"attention_mask\"]\n",
    "            for idx in range(bs)\n",
    "        ]\n",
    "        features[\"targets\"] = [\n",
    "            answer_choices_texts[idx].index(t)\n",
    "            for idx, t in enumerate(target_texts)\n",
    "        ]\n",
    "        features['indices'] = [\n",
    "            indices[idx] for idx in range(bs)\n",
    "        ]\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    ### preprocess dataset ###\n",
    "    with accelerator.main_process_first():\n",
    "        print('preparing dataset ...')\n",
    "        eval_dataset = raw_datasets.map(\n",
    "            preprocess_function_step2, batched=True, remove_columns=column_names\n",
    "        )\n",
    "\n",
    "    # Log a few random samples from the eval set:\n",
    "    for index in random.sample(range(len(eval_dataset)), 3):\n",
    "        logger.info(f\"Sample {index} of the training set: {eval_dataset[index]}.\")\n",
    "\n",
    "    # DataLoaders creation:\n",
    "    if pad_to_max_length:\n",
    "        # If padding was already done ot max length, we use the default data collator that will just convert everything\n",
    "        # to tensors.\n",
    "        data_collator = default_data_collator\n",
    "    else:\n",
    "        # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n",
    "        # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n",
    "        # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n",
    "        data_collator = DataCollatorForMultipleChoice(\n",
    "            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n",
    "        )\n",
    "    \n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=per_device_eval_batch_size)\n",
    "\n",
    "\n",
    "    # Use the device given by the `accelerator` object.\n",
    "    if not if_parallelize:\n",
    "        model.to(accelerator.device)\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    eval_dataloader = accelerator.prepare(eval_dataloader)\n",
    "\n",
    "    # Metrics\n",
    "    metric = load_metric(\"accuracy\")\n",
    "\n",
    "    # Eval!\n",
    "    total_batch_size = per_device_eval_batch_size * accelerator.num_processes\n",
    "\n",
    "    logger.info(\"***** Running step two with augmentated samples *****\")\n",
    "    logger.info(f\"  Num examples = {len(eval_dataset)}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {per_device_eval_batch_size}\")\n",
    "    logger.info(f\"  Total eval batch size (w. parallel, distributed) = {total_batch_size}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(len(eval_dataloader)), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            predictions, confidences = model(batch)\n",
    "\n",
    "        metric.add_batch(\n",
    "            predictions=accelerator.gather(predictions),\n",
    "            references=accelerator.gather(batch[\"targets\"]),\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    accelerator.print(f\"Result step one: {eval_metric}\")\n",
    "\n",
    "    results = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"dataset_config_name\": dataset_config_name,\n",
    "        \"template_name\": template_name,\n",
    "        \"evaluation\": eval_metric,\n",
    "        \"retrieval_database\": None\n",
    "    }\n",
    "    all_results_step2.append(results)\n",
    "\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if output_dir is not None:\n",
    "        output_name = f\"results_step1__{dataset_name}__{dataset_config_name}.json\"\n",
    "        output_name = output_name.replace('/','_')\n",
    "        with open(os.path.join(output_dir, output_name), \"w\") as f:\n",
    "            json.dump(all_results_step1, f, indent=4)\n",
    "\n",
    "if accelerator.is_main_process:\n",
    "    if output_dir is not None:\n",
    "        output_name = f\"results_step2__{dataset_name}__{dataset_config_name}.json\"\n",
    "        output_name = output_name.replace('/','_')\n",
    "        with open(os.path.join(output_dir, output_name), \"w\") as f:\n",
    "            json.dump(all_results_step2, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2170968be0d8cf7da3f54475ddf88c7d4b4de4533e35361df492f15758e1b24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('t-zero')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
