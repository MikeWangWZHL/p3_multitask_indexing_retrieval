{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets, load_dataset\n",
    "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# from multiprocess import set_start_method\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# def set_up_device():\n",
    "#     # single gpu\n",
    "#     gpu_index = 1\n",
    "#     if torch.cuda.is_available():\n",
    "#         dev = f\"cuda:{gpu_index}\"\n",
    "#     else:\n",
    "#         dev = \"cpu\"\n",
    "#     return gpu_index, torch.device(dev)\n",
    "#     # return gpu_index, torch.device(\"cpu\")\n",
    "\n",
    "def load_multitask_datasets(dataset_disk_paths):\n",
    "    print(\"picked datasets x prompt_template number:\", len(dataset_disk_paths))\n",
    "    datasets = []\n",
    "    print('loading datasets train split ...')\n",
    "    for p in tqdm(dataset_disk_paths):\n",
    "        loaded_dataset = load_from_disk(p)\n",
    "        if \"train\" in loaded_dataset:\n",
    "            loaded_dataset_train_split = loaded_dataset['train']\n",
    "        else:\n",
    "            print(f'INFO: no train split found, using entire dataset: {p}')\n",
    "            loaded_dataset_train_split = loaded_dataset\n",
    "\n",
    "        datasets.append(loaded_dataset_train_split)\n",
    "    concatenated = concatenate_datasets(datasets)\n",
    "    print(\"concatenated dataset:\", concatenated)\n",
    "    return concatenated\n",
    "\n",
    "def load_single_dataset(dataset_path):\n",
    "    loaded_dataset = load_from_disk(dataset_path)\n",
    "    if \"train\" in loaded_dataset:\n",
    "        loaded_dataset_train_split = loaded_dataset['train']\n",
    "    else:\n",
    "        print(f'INFO: no train split found, using entire dataset: {dataset_path}')\n",
    "        loaded_dataset_train_split = loaded_dataset\n",
    "    return loaded_dataset_train_split\n",
    "\n",
    "@torch.no_grad()\n",
    "def setup_retriever(saved_index_dir, device):\n",
    "    # load dataset and index\n",
    "    index_path = os.path.join(saved_index_dir,'index.faiss')\n",
    "    config_json = os.path.join(saved_index_dir,'config.json')\n",
    "    \n",
    "    config = json.load(open(config_json))\n",
    "    key_name, dataset_paths = config['key_name'], config['dataset_paths']\n",
    "    ds_with_embeddings = load_multitask_datasets(dataset_paths)\n",
    "    ds_with_embeddings.load_faiss_index('embeddings', index_path, device=-1)\n",
    "\n",
    "    q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "    q_encoder.to(device)\n",
    "    q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "    return ds_with_embeddings, q_encoder, q_tokenizer\n",
    "\n",
    "@torch.no_grad()\n",
    "def retrieve(\n",
    "        ds_with_embeddings,\n",
    "        q_encoder,\n",
    "        q_tokenizer,\n",
    "        question,\n",
    "        device,\n",
    "        topk=3\n",
    "    ):\n",
    "    question_embedding = q_encoder(**q_tokenizer(question, return_tensors=\"pt\", truncation=True).to(device))[0][0].cpu().numpy()\n",
    "    scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', question_embedding, k=topk)\n",
    "    return scores, retrieved_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "\n",
      "Given the following passage\n",
      "\n",
      "\"When he became First Consul and later Emperor, Napoleon eschewed his general's uniform and habitually wore the simple green colonel uniform (non-Hussar) of a colonel of the Chasseur à Cheval of the Imperial Guard, the regiment that often served as his personal escort, with a large bicorne. He also habitually wore (usually on Sundays) the blue uniform of a colonel of the Imperial Guard Foot Grenadiers (blue with white facings and red cuffs). He also wore his Légion d'honneur star, medal and ribbon, and the Order of the Iron Crown decorations, white French-style culottes and white stockings. This was in contrast to the gorgeous and complex uniforms with many decorations of his marshals and those around him.\",\n",
      "\n",
      "answer the following question. Note that the answer is present within the text.\n",
      "\n",
      "Question: What jewelry like accessories did he wear? \n",
      "---------------------------\n",
      "Dataset({\n",
      "    features: ['answer_choices', 'inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "Are the following two sentences \"equivalent\" or \"not equivalent\"?\n",
      "Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
      "\n",
      "---------------------------\n",
      "Dataset({\n",
      "    features: ['inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized', 'answer_choices'],\n",
      "    num_rows: 13668\n",
      "})\n",
      "\n",
      "Given the following passage\n",
      "\n",
      "\"When he became First Consul and later Emperor, Napoleon eschewed his general's uniform and habitually wore the simple green colonel uniform (non-Hussar) of a colonel of the Chasseur à Cheval of the Imperial Guard, the regiment that often served as his personal escort, with a large bicorne. He also habitually wore (usually on Sundays) the blue uniform of a colonel of the Imperial Guard Foot Grenadiers (blue with white facings and red cuffs). He also wore his Légion d'honneur star, medal and ribbon, and the Order of the Iron Crown decorations, white French-style culottes and white stockings. This was in contrast to the gorgeous and complex uniforms with many decorations of his marshals and those around him.\",\n",
      "\n",
      "answer the following question. Note that the answer is present within the text.\n",
      "\n",
      "Question: What jewelry like accessories did he wear? \n",
      "Are the following two sentences \"equivalent\" or \"not equivalent\"?\n",
      "Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
      "\n",
      "range(10000, 13668)\n",
      "Dataset({\n",
      "    features: ['inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized', 'answer_choices'],\n",
      "    num_rows: 3668\n",
      "})\n",
      "Are the following two sentences \"equivalent\" or \"not equivalent\"?\n",
      "Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .\n",
      "Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds = load_single_dataset('/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_answer_the_following_q')\n",
    "print(ds)\n",
    "print(ds[0]['inputs_pretokenized'])\n",
    "print('---------------------------')\n",
    "\n",
    "ds2 = load_single_dataset(\"/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/glue_mrpc_equivalent\")\n",
    "print(ds2)\n",
    "print(ds2[0]['inputs_pretokenized'])\n",
    "print('---------------------------')\n",
    "concatenated_ds = concatenate_datasets([ds,ds2])\n",
    "print(concatenated_ds)\n",
    "print(concatenated_ds[0]['inputs_pretokenized'])\n",
    "print(concatenated_ds[10000]['inputs_pretokenized'])\n",
    "\n",
    "print(range(10000,concatenated_ds.num_rows))\n",
    "concatenated_ds = concatenated_ds.select(range(10000, concatenated_ds.num_rows))\n",
    "print(concatenated_ds)\n",
    "print(concatenated_ds[0]['inputs_pretokenized'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picked datasets x prompt_template number: 5\n",
      "loading datasets train split ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 17.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated dataset: Dataset({\n",
      "    features: ['inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized'],\n",
      "    num_rows: 50000\n",
      "})\n",
      "['/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_answer_the_following_q', '/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_generate_question', '/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_tell_what_it_is', '/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_based_on', '/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_question_context_answer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tobe_removed = [\"/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_answer_the_following_q\",\n",
    "\"/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_generate_question\",\n",
    "\"/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_tell_what_it_is\",\n",
    "\"/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_based_on\",\n",
    "\"/cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/adversarial_qa_droberta_question_context_answer\"]\n",
    "tobe_removed_ds = load_multitask_datasets(tobe_removed)\n",
    "print(tobe_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_index_dir_path=\"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/output_indexing/p3_subset_6_3-part-1\"\n",
    "\n",
    "# load dataset and index\n",
    "index_path = os.path.join(saved_index_dir_path,'index_.faiss')\n",
    "config_json = os.path.join(saved_index_dir_path,'config_.json')\n",
    "\n",
    "config = json.load(open(config_json))\n",
    "key_name, dataset_paths = config['key_name'], config['dataset_paths']\n",
    "ds_with_embeddings = load_multitask_datasets(dataset_paths)\n",
    "ds_with_embeddings.load_faiss_index('embeddings', index_path)\n",
    "print(range(50000, ds_with_embeddings.num_rows))\n",
    "ds_with_embeddings = ds_with_embeddings.select(range(50000, ds_with_embeddings.num_rows))\n",
    "print(ds_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_index_dir = saved_index_dir_path\n",
    "saving_index_path = os.path.join(output_index_dir, \"index.faiss\")\n",
    "# saving_config_json_path = os.path.join(output_dir, \"config.json\")\n",
    "print('saving faiss index to ', saving_index_path)\n",
    "ds_with_embeddings.save_faiss_index('embeddings', saving_index_path)\n",
    "\n",
    "output_dataset_dir = \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/indexed_datasets\"\n",
    "print('saving dataset ...')\n",
    "saving_dataset_path = os.path.join(output_dataset_dir, os.path.basename(saved_index_dir_path))\n",
    "ds_with_embeddings.save_to_disk(saving_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "picked datasets x prompt_template number: 89\n",
      "loading datasets train split ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 12/89 [00:00<00:03, 22.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/glue_qqp_answer_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 14/89 [00:20<03:26,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/glue_qqp_meaning_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 15/89 [00:31<04:55,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/glue_qqp_duplicate_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 16/89 [00:42<06:25,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/glue_qqp_duplicate_or_not_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 17/89 [00:42<05:04,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/glue_qqp_quora_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 20/89 [00:55<04:36,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/glue_qqp_same_thing_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 85/89 [01:32<00:12,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/dbpedia_14_given_a_choice_of_categories__sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 86/89 [01:55<00:19,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/dbpedia_14_given_list_what_category_does_the_paragraph_belong_to_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 87/89 [02:17<00:18,  9.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/dbpedia_14_pick_one_category_for_the_following_text_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 89/89 [02:35<00:00,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: no train split found, using entire dataset: /cephfs/user/mikeeewang/summer_22/workspace/data/p3_subset/dbpedia_14_given_a_list_of_category_what_does_the_title_belong_to_sampled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concatenated dataset: Dataset({\n",
      "    features: ['inputs', 'inputs_pretokenized', 'targets', 'targets_pretokenized', 'answer_choices'],\n",
      "    num_rows: 6421376\n",
      "})\n",
      "save to disk ...  p3_subset_6_3-part-1\n"
     ]
    }
   ],
   "source": [
    "saved_index_dir_paths = [\n",
    "    # \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/output_indexing/p3_subset_6_1\",\n",
    "    \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/output_indexing/p3_subset_6_3-part-1\",\n",
    "    # \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/output_indexing/p3_subset_6_3-part-2\"\n",
    "]\n",
    "\n",
    "output_dataset_dir = \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/indexed_datasets\"\n",
    "\n",
    "ds_list = []\n",
    "for saved_index_dir_path in saved_index_dir_paths:\n",
    "    # load dataset and index\n",
    "    index_path = os.path.join(saved_index_dir_path,'index.faiss')\n",
    "    config_json = os.path.join(saved_index_dir_path,'config.json')\n",
    "\n",
    "    config = json.load(open(config_json))\n",
    "    key_name, dataset_paths = config['key_name'], config['dataset_paths']\n",
    "    ds_with_embeddings = load_multitask_datasets(dataset_paths)\n",
    "    ds_with_embeddings.save_to_disk(os.path.join(output_dataset_dir, os.path.basename(saved_index_dir_path)))\n",
    "    print('save to disk ... ', os.path.basename(saved_index_dir_path))\n",
    "    ds_with_embeddings.load_faiss_index('embeddings', index_path)\n",
    "    ds_list.append(ds_with_embeddings)\n",
    "\n",
    "# ds_list.append(load_from_disk(os.path.join(output_dataset_dir,\"p3_subset_6_3-part-1\")))\n",
    "\n",
    "# concatenate\n",
    "# concatenated_ds = concatenate_datasets(ds_list)\n",
    "# print('saving concatenated dataset ...')\n",
    "# saving_dataset_path = os.path.join(output_dataset_dir, \"concatenated_6-1_6-3-part-2\")\n",
    "# ds_with_embeddings.save_to_disk(saving_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: /cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/indexed_datasets/p3_subset_6_1\n",
      "index_path: /cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/output_indexing/p3_subset_6_1/index.faiss\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_faiss_index() got an unexpected keyword argument 'deivce'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/retrieval.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/retrieval.ipynb#ch0000006vscode-remote?line=12'>13</a>\u001b[0m index_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(saved_index_dir, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(shard), \u001b[39m\"\u001b[39m\u001b[39mindex.faiss\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/retrieval.ipynb#ch0000006vscode-remote?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mindex_path:\u001b[39m\u001b[39m'\u001b[39m,index_path)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/retrieval.ipynb#ch0000006vscode-remote?line=14'>15</a>\u001b[0m reloaded\u001b[39m.\u001b[39;49mload_faiss_index(\u001b[39m'\u001b[39;49m\u001b[39membeddings\u001b[39;49m\u001b[39m'\u001b[39;49m, index_path, deivce\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/retrieval.ipynb#ch0000006vscode-remote?line=15'>16</a>\u001b[0m reloaded_ds_list\u001b[39m.\u001b[39mappend(reloaded)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2243413331227d/cephfs/user/mikeeewang/summer_22/code/t-zero/evaluation/retrieval.ipynb#ch0000006vscode-remote?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(reloaded)\n",
      "\u001b[0;31mTypeError\u001b[0m: load_faiss_index() got an unexpected keyword argument 'deivce'"
     ]
    }
   ],
   "source": [
    "# try loading the entire dataset:\n",
    "concatenated_shard_paths = [\n",
    "    \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/indexed_datasets/p3_subset_6_1\",\n",
    "    \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/indexed_datasets/p3_subset_6_3-part-1\",\n",
    "    \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/indexed_datasets/p3_subset_6_3-part-2\"\n",
    "]\n",
    "saved_index_dir = \"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/output_indexing\"\n",
    "\n",
    "reloaded_ds_list = []\n",
    "for shard in concatenated_shard_paths:\n",
    "    print(\"loading:\",shard)\n",
    "    reloaded = load_from_disk(shard)\n",
    "    index_path = os.path.join(saved_index_dir, os.path.basename(shard), \"index.faiss\")\n",
    "    print('index_path:',index_path)\n",
    "    reloaded.load_faiss_index('embeddings', index_path, device=-1)\n",
    "    reloaded_ds_list.append(reloaded)\n",
    "    print(reloaded)\n",
    "\n",
    "ds_with_embeddings = concatenate_datasets(reloaded_ds_list)\n",
    "print(ds_with_embeddings)\n",
    "\n",
    "for ds in reloaded_ds_list:\n",
    "    print(ds.list_indexes())\n",
    "\n",
    "print(ds_with_embeddings.list_indexes())\n",
    "\n",
    "# ds_with_embeddings.save_faiss_index('embeddings', 'tmp.faiss')\n",
    "\n",
    "# # try faiss\n",
    "# retriever_device = torch.device(\"cuda:7\")\n",
    "# q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "# q_encoder.to(retriever_device)\n",
    "# q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "\n",
    "# input = \"hello world\"\n",
    "# scores, retreived_examples = retrieve(\n",
    "#     ds_with_embeddings,\n",
    "#     q_encoder,\n",
    "#     q_tokenizer,\n",
    "#     question=input,\n",
    "#     device=retriever_device,\n",
    "#     topk=5\n",
    "# )\n",
    "# print(retreived_examples[\"inputs_pretokenized\"])\n",
    "# print(retreived_examples[\"targets_pretokenized\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saved_index_dir_path=\"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/output_indexing/p3_subset_6_1\"\n",
    "\n",
    "# load dataset and index\n",
    "index_path = os.path.join(saved_index_dir_path,'index.faiss')\n",
    "config_json = os.path.join(saved_index_dir_path,'config.json')\n",
    "\n",
    "config = json.load(open(config_json))\n",
    "key_name, dataset_paths = config['key_name'], config['dataset_paths']\n",
    "ds_with_embeddings = load_multitask_datasets(dataset_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/dpr-question_encoder-single-nq-base were not used when initializing DPRQuestionEncoder: ['question_encoder.bert_model.pooler.dense.bias', 'question_encoder.bert_model.pooler.dense.weight']\n",
      "- This IS expected if you are initializing DPRQuestionEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DPRQuestionEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "retrieve_num = 1\n",
    "concat_num = 1\n",
    "retriever_device = torch.device(\"cuda:1\")\n",
    "\n",
    "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
    "q_encoder.to(retriever_device)\n",
    "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever_device = torch.device(\"cuda:1\")\n",
    "# retrieve_num = 1\n",
    "# concat_num = 1\n",
    "# saved_index_dir_path=\"/cephfs/user/mikeeewang/summer_22/code/t-zero/retrieval_indexing/output_indexing/p3_subset_6_1\"\n",
    "# ds_with_embeddings, q_encoder, q_tokenizer = setup_retriever(\n",
    "#     saved_index_dir=saved_index_dir_path,\n",
    "#     device=retriever_device\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jubilation greets boy #39;s miracle rescue ALMOST four days after being engulfed by a collapsing hill, two-year-old Yuta Minagawa was lifted alive yesterday afternoon from the wreckage of his mother #39;s car. \\nIs this a piece of news regarding world politics, sports, business, or science and technology? ', '\\nI know that the answer to the question \"What was Walt Disney\\'s last name?\" is in \"Several works from the Golden Age of Animation matched the action to classical music. Notable examples are Walt Disney\\'s Fantasia, Tom and Jerry\\'s Johann Mouse, and Warner Bros.\\' Rabbit of Seville and What\\'s Opera, Doc?.\". Can you tell me what it is? ', 'Jubilation greets boy #39;s miracle rescue ALMOST four days after being engulfed by a collapsing hill, two-year-old Yuta Minagawa was lifted alive yesterday afternoon from the wreckage of his mother #39;s car. \\n\\nWhich of the following sections of a newspaper would this article likely appear in? World News, Sports, Business, or Science and Technology? ', 'World waits and prays as family makes appeal to kidnappers THEY have been riven with terror as the world watches the plight of Ken Bigley. But yesterday his family came together to reach out to his captors with emotional pleas for mercy. \\nIs this a piece of news regarding world politics, sports, business, or science and technology? ', 'World waits and prays as family makes appeal to kidnappers THEY have been riven with terror as the world watches the plight of Ken Bigley. But yesterday his family came together to reach out to his captors with emotional pleas for mercy. \\nIs this a piece of news regarding world politics, sports, business, or science and technology? ']\n",
      "[' \\nWorld politics', '\\n\\nDisney\\n', ' \\nWorld News', ' \\nWorld politics', ' \\nWorld politics']\n"
     ]
    }
   ],
   "source": [
    "input = \"hello world\"\n",
    "scores, retreived_examples = retrieve(\n",
    "    ds_with_embeddings,\n",
    "    q_encoder,\n",
    "    q_tokenizer,\n",
    "    question=input,\n",
    "    device=retriever_device,\n",
    "    topk=5\n",
    ")\n",
    "print(retreived_examples[\"inputs_pretokenized\"])\n",
    "print(retreived_examples[\"targets_pretokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2170968be0d8cf7da3f54475ddf88c7d4b4de4533e35361df492f15758e1b24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('t-zero')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
