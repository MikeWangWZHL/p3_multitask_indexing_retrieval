{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, OPTForCausalLM, GPTJForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from promptsource.templates import DatasetTemplates\n",
    "import copy\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def gen(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        prompt, \n",
    "        input_device, \n",
    "        num_return_sequences = 5, \n",
    "        do_sample = True,\n",
    "        max_length = 32,\n",
    "        temperature = 0.9\n",
    "    ):\n",
    "    # inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(input_device)\n",
    "    # generation_output = model.generate(inputs, return_dict_in_generate = True)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\",truncation=True,padding=True).to(input_device)\n",
    "    generation_output = model.generate(\n",
    "        inputs.input_ids,\n",
    "        no_repeat_ngram_size = 3,\n",
    "        temperature=temperature,\n",
    "        max_length=max_length,\n",
    "        do_sample=do_sample,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        output_scores = True,\n",
    "        return_dict_in_generate = True\n",
    "    )\n",
    "    response = tokenizer.batch_decode(generation_output['sequences'], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    return generation_output, response\n",
    "\n",
    "@torch.no_grad()\n",
    "def cal_log_perplexity_generate(generation_output):\n",
    "    # print(generation_output.keys())\n",
    "    sequence_num, generated_sequence_length = generation_output['sequences'].size()[0] ,len(generation_output['scores'])\n",
    "    print(\"generate length:\", generated_sequence_length)\n",
    "    generation_output['scores'] = torch.stack(list(generation_output['scores']), dim=0)\n",
    "    # print(generation_output['scores'].size())\n",
    "    # print(f'sequence num = {sequence_num}, generated_sequence_length = {generated_sequence_length}')\n",
    "    \n",
    "    perp = []\n",
    "    for i in range(sequence_num):\n",
    "        generated_squence_ids = generation_output['sequences'][i][-generated_sequence_length:]\n",
    "        scores = generation_output['scores'][:,i,:]\n",
    "        log_softmax_scores = F.log_softmax(scores, dim=1)\n",
    "        # print(scores.size(),log_softmax_scores.size())\n",
    "        # print(scores[0][:10], log_softmax_scores[0][:10])\n",
    "        assert scores.size()[0] == generated_squence_ids.size()[0]\n",
    "        generated_squence_ids = generated_squence_ids.cpu().numpy()\n",
    "        log_softmax_scores = log_softmax_scores.cpu().numpy()\n",
    "        log_sum = 0\n",
    "        for j in range(len(generated_squence_ids)):\n",
    "            idx = generated_squence_ids[j]\n",
    "            log_sum += log_softmax_scores[j][idx]\n",
    "        perp.append(np.exp((-1/generated_sequence_length)*log_sum))\n",
    "    return perp\n",
    "\n",
    "def cal_log_perplexity_decode(logits, input_ids):\n",
    "    perp = []\n",
    "    for i in range(input_ids.size()[0]):\n",
    "        generated_squence_ids = input_ids[i]\n",
    "        n = generated_squence_ids.size()[0]\n",
    "        scores = logits[i]\n",
    "        log_softmax_scores = F.log_softmax(scores, dim=1)\n",
    "        assert scores.size()[0] == n\n",
    "        generated_squence_ids = generated_squence_ids.cpu().numpy()\n",
    "        log_softmax_scores = log_softmax_scores.cpu().numpy()\n",
    "        log_sum = 0\n",
    "        for j in range(len(generated_squence_ids)):\n",
    "            idx = generated_squence_ids[j]\n",
    "            log_sum += log_softmax_scores[j][idx]\n",
    "        perp.append(np.exp((-1/n)*log_sum))\n",
    "    return perp\n",
    "\n",
    "@torch.no_grad()\n",
    "def rerank(input_text, tokenizer, model, input_device):\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True).to(input_device)\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    # outputs = model(**inputs) #TODO\n",
    "    logits = outputs.logits\n",
    "    perps = cal_log_perplexity_decode(logits, inputs.input_ids)\n",
    "    assert len(perps) == len(input_text)\n",
    "    ranked = sorted([ (i, input_text[i], perps[i]) for i in range(len(perps))], key = lambda x: x[2])\n",
    "    return ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading main model: gpt2-large\n"
     ]
    }
   ],
   "source": [
    "#### GPT-2 ####\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1,2,3,4,5,6,7\"\n",
    "main_model_name = \"gpt2-large\"\n",
    "\n",
    "print(f\"loading main model: {main_model_name}\")\n",
    "### set up model ###\n",
    "main_model = AutoModelForCausalLM.from_pretrained(main_model_name)\n",
    "main_model.parallelize()\n",
    "\n",
    "### set up tokenizer ###\n",
    "main_tokenizer = AutoTokenizer.from_pretrained(main_model_name)\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "\n",
    "### set input device as ###\n",
    "main_input_device = main_model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GPT-J ####\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1,2,3,4,5,6,7\"\n",
    "main_model_name = \"EleutherAI/gpt-j-6B\"\n",
    "\n",
    "print(f\"loading main model: {main_model_name}\")\n",
    "### set up model ###\n",
    "main_model = GPTJForCausalLM.from_pretrained(main_model_name)\n",
    "main_model.parallelize()\n",
    "\n",
    "### set input device as ###\n",
    "main_input_device = main_model.device\n",
    "\n",
    "### set up tokenizer ###\n",
    "main_tokenizer = AutoTokenizer.from_pretrained(main_model_name)\n",
    "main_tokenizer.pad_token = main_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### T0 ####\n",
    "\n",
    "### set up device\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0,1,2,3,4,5,6,7\"\n",
    "# main_model_name = \"bigscience/T0-3B\"\n",
    "main_model_name = \"bigscience/T0\"\n",
    "# main_model_name = \"bigscience/T0p\" # plus\n",
    "# main_model_name = \"bigscience/T0pp\" # plus plus\n",
    "\n",
    "### set up tokenizer ###\n",
    "main_tokenizer = AutoTokenizer.from_pretrained(main_model_name)\n",
    "# main_tokenizer.pad_token = main_tokenizer.eos_token\n",
    "\n",
    "print(f\"loading main model: {main_model_name}\")\n",
    "### set up model ###\n",
    "main_model = AutoModelForSeq2SeqLM.from_pretrained(main_model_name)\n",
    "main_model.parallelize()\n",
    "\n",
    "### set input device as ###\n",
    "main_input_device = main_model.device\n",
    "print('model main device:',main_input_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts from retrieval result json\n",
    "def load_prompts_from_retrieval_result():\n",
    "    retrieval_results = json.load(open(\"/cephfs/user/mikeeewang/summer_22/code/t-zero/output/T0_3B__super_glue__wic_debug/retrieval_results.json\"))\n",
    "    all_prompts = []\n",
    "    tempalte_names = list(retrieval_results.keys())\n",
    "    for i in range(len(retrieval_results[tempalte_names[0]])):\n",
    "        prompts = []\n",
    "        for template_name in tempalte_names:\n",
    "            items = retrieval_results[template_name]\n",
    "            prompts.append(items[i]['input'])\n",
    "        all_prompts.append(prompts)\n",
    "    return tempalte_names, all_prompts\n",
    "tempalte_names, all_prompts = load_prompts_from_retrieval_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5392ecdfa4324c7585cb893e080b5b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.38k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de73b7737f0489d898b8a29e60af385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cos_e/v1.11 (download: 6.23 MiB, generated: 2.91 MiB, post-processed: Unknown size, total: 9.14 MiB) to /data2/mikeeewang/.cache/huggingface/cos_e/v1.11/1.11.0/e8dc57a5b321a2a97063efb8d316d6d8a0d9a2d3a392dafc913e55bed42736d2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a031b43548bb489bb5ac5cf97196d1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a846721f544403aac68c97525d4f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.79M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72d26cf341c24879998114cb785cb5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/472k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc356caa9f1e43bc8ada8896d674d1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/423k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055ed6ff1f1d4ece88ebbc7a0a488e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2cb7db713c42e4b432fc17238357ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca7b706952f46b19ad87aa6a69644f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/67.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1686d7c1f947e98fed62b8cd17d6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/539k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea67364507b54679957795019de294f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e21971f6e34f4084f289befcf5428a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7efecd3058d048569ae6fb7fe9ddb354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cos_e downloaded and prepared to /data2/mikeeewang/.cache/huggingface/cos_e/v1.11/1.11.0/e8dc57a5b321a2a97063efb8d316d6d8a0d9a2d3a392dafc913e55bed42736d2. Subsequent calls will reuse this data.\n",
      "\n",
      "question_description_option_text\n",
      "question_description_option_id\n",
      "rationale\n",
      "question_option_description_text\n",
      "aligned_with_common_sense\n",
      "description_question_option_id\n",
      "explain_why_human\n",
      "generate_explanation_given_text\n",
      "description_question_option_text\n",
      "i_think\n",
      "question_option_description_id\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# dataset_name = \"super_glue\"\n",
    "# dataset_config_name = \"wic\"\n",
    "dataset_name = \"cos_e\"\n",
    "dataset_config_name =\"v1.11\"\n",
    "template_name = None\n",
    "\n",
    "if dataset_name == \"anli\":\n",
    "    raw_datasets = load_dataset(dataset_name, split=dataset_config_name)\n",
    "else:\n",
    "    raw_datasets = load_dataset(dataset_name, dataset_config_name, split=\"validation\")\n",
    "\n",
    "prompts_wrapper = DatasetTemplates(\n",
    "    f\"{dataset_name}\"\n",
    "    if dataset_config_name is None\n",
    "    else f\"{dataset_name}/{dataset_config_name}\"\n",
    ")\n",
    "\n",
    "\n",
    "dummy_example = copy.deepcopy(raw_datasets[0])\n",
    "dummy_example['word'] = \"N/A\"\n",
    "dummy_example['sentence1'] = \"N/A\"\n",
    "dummy_example['sentence2'] = \"N/A\"\n",
    "\n",
    "# example = raw_datasets[0]\n",
    "example = dummy_example\n",
    "\n",
    "# print(example)\n",
    "tempalte_names = []\n",
    "prompts = []\n",
    "print()\n",
    "for t in list(prompts_wrapper.templates.values()):\n",
    "    # print(\"template name:\", t.name)\n",
    "    input, target = t.apply(example)\n",
    "    # print(\"INPUT:\", input, \"\\nTARGET:\", target, \"\\n\")\n",
    "    prompts.append(input)\n",
    "    tempalte_names.append(t.name)\n",
    "    print(t.name)\n",
    "all_prompts = [prompts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['question-context-meaning-with-label', 'question-context-meaning', 'grammar_homework', 'affirmation_true_or_false', 'GPT-3-prompt', 'same_sense', 'question-context', 'GPT-3-prompt-with-label', 'polysemous', 'similar-sense'] 1 ['Does the word \"N/A\" have the same meaning in these two sentences? Yes, No?\\nN/A\\nN/A', 'Does the word \"N/A\" have the same meaning in these two sentences?\\nN/A\\nN/A', 'Homework\\n\\nDecide whether the word \"N/A\" is used with the same meaning in the two following sentences. Answer by yes or no.\\nN/A\\nN/A', 'Sentence A: N/A\\nSentence B: N/A\\n\\n\"N/A\" has a similar meaning in sentences A and B. True or False?', \"N/A\\nN/A\\nQuestion: Is the word 'N/A' used in the same sense in the two sentences above?\", 'Sentence 1: N/A\\nSentence 2: N/A\\n\\nDetermine whether the word \"N/A\" is used in the same sense in both sentences. Yes or no?', \"Determine if the word 'N/A' is used in the same way in the two sentences below. \\nN/A\\nN/A\", \"N/A\\nN/A\\nQuestion: Is the word 'N/A' used in the same sense in the two sentences above? Yes, No?\", 'The word \"N/A\" has multiple meanings. Does it have the same meaning in sentences 1 and 2? Yes or no?\\n\\nSentence 1: N/A\\nSentence 2: N/A', 'N/A\\nN/A\\nSimilar sense of N/A?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 'Homework\\n\\nDecide whether the word \"N/A\" is used with the same meaning in the two following sentences. Answer by yes or no.\\nN/A\\nN/A', 13006.41474802519), (7, \"N/A\\nN/A\\nQuestion: Is the word 'N/A' used in the same sense in the two sentences above? Yes, No?\", 18160.680249904097), (6, \"Determine if the word 'N/A' is used in the same way in the two sentences below. \\nN/A\\nN/A\", 25010.554855909813), (5, 'Sentence 1: N/A\\nSentence 2: N/A\\n\\nDetermine whether the word \"N/A\" is used in the same sense in both sentences. Yes or no?', 27375.047994202898), (4, \"N/A\\nN/A\\nQuestion: Is the word 'N/A' used in the same sense in the two sentences above?\", 29543.16402057159), (3, 'Sentence A: N/A\\nSentence B: N/A\\n\\n\"N/A\" has a similar meaning in sentences A and B. True or False?', 32327.70038905803), (0, 'Does the word \"N/A\" have the same meaning in these two sentences? Yes, No?\\nN/A\\nN/A', 33362.95340983403), (8, 'The word \"N/A\" has multiple meanings. Does it have the same meaning in sentences 1 and 2? Yes or no?\\n\\nSentence 1: N/A\\nSentence 2: N/A', 39929.974240060255), (1, 'Does the word \"N/A\" have the same meaning in these two sentences?\\nN/A\\nN/A', 43662.95408265034), (9, 'N/A\\nN/A\\nSimilar sense of N/A?', 54866.08366125715)]\n",
      "defaultdict(<class 'int'>, {'grammar_homework': 1})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(tempalte_names, len(all_prompts), all_prompts[0])\n",
    "template_name_to_ranks = defaultdict(int)\n",
    "\n",
    "for prompts in tqdm(all_prompts):\n",
    "    reranked_prompts = rerank(prompts, main_tokenizer, main_model, main_input_device)\n",
    "    top_1_index = reranked_prompts[0][0]\n",
    "    template_name_to_ranks[tempalte_names[top_1_index]] += 1\n",
    "    # print(prompts)\n",
    "    print(reranked_prompts)\n",
    "    print(template_name_to_ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2170968be0d8cf7da3f54475ddf88c7d4b4de4533e35361df492f15758e1b24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('t-zero')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
